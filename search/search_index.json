{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#aimet-ml","title":"aimet-ml","text":"<p>Python package of frequently used modules for ML developments in AIMET.</p> <ul> <li>Documentation: https://aimet-tech.github.io/aimet-ml</li> <li>GitHub: https://github.com/aimet-tech/aimet-ml</li> <li>PyPI: https://pypi.org/project/aimet-ml/</li> <li>Free software: MIT</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#063---2024-03-07","title":"[0.6.3] - 2024-03-07","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Correct the default value of <code>val_fraction</code> in the function <code>split_dataset_v2</code>.</li> </ul>"},{"location":"changelog/#062---2024-03-06","title":"[0.6.2] - 2024-03-06","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Separate description lines from summary lines in the docstrings.</li> </ul>"},{"location":"changelog/#061---2024-03-06","title":"[0.6.1] - 2024-03-06","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Update the docstrings to clarify the distinction between <code>split_dataset</code> and <code>split_dataset_v2</code>.</li> </ul>"},{"location":"changelog/#060---2024-03-06","title":"[0.6.0] - 2024-03-06","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add a new data splitting functions, <code>split_dataset_v2</code>.</li> </ul>"},{"location":"changelog/#050---2024-01-18","title":"[0.5.0] - 2024-01-18","text":""},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Update transformers version requirement to ^4.36.2.</li> <li>Move transformers and torch to a new extra dependency section named \"transformers\".</li> </ul>"},{"location":"changelog/#044---2023-12-22","title":"[0.4.4] - 2023-12-22","text":""},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Loosen pandas version requirement to ^1.5.3.</li> </ul>"},{"location":"changelog/#043---2023-11-02","title":"[0.4.3] - 2023-11-02","text":""},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Update mkdocs.yml to display documents for the new modules.</li> </ul>"},{"location":"changelog/#042---2023-11-02","title":"[0.4.2] - 2023-11-02","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add .env.template</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Update test coverage config.</li> </ul>"},{"location":"changelog/#041---2023-11-02","title":"[0.4.1] - 2023-11-02","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Update documents for the recently added functions.</li> </ul>"},{"location":"changelog/#040---2023-11-02","title":"[0.4.0] - 2023-11-02","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Add data splitting functions.</li> <li>Add evaluation metric report functions.</li> </ul>"},{"location":"changelog/#033---2023-10-30","title":"[0.3.3] - 2023-10-30","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Add trim_tokens for text processing.</li> </ul>"},{"location":"changelog/#032---2023-10-27","title":"[0.3.2] - 2023-10-27","text":""},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Edit type hints</li> <li>Convert variables to appropriate types.</li> </ul>"},{"location":"changelog/#031---2023-10-27","title":"[0.3.1] - 2023-10-27","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Add docstrings and type hints to the test scripts of the new utility modules.</li> </ul>"},{"location":"changelog/#030---2023-10-26","title":"[0.3.0] - 2023-10-26","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Add utilities for training pipeline.</li> <li>aws utils</li> <li>git utils</li> <li>io utils</li> <li>wandb utils</li> </ul>"},{"location":"changelog/#020---2023-08-28","title":"[0.2.0] - 2023-08-28","text":""},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>Change layout of module document.</li> </ul>"},{"location":"changelog/#011---2023-08-26","title":"[0.1.1] - 2023-08-26","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Cover tests for more modules.</li> </ul>"},{"location":"changelog/#010---2023-08-26","title":"[0.1.0] - 2023-08-26","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>First release (pre-release) on PyPI.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/aimet-tech/aimet-ml/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>aimet-ml could always use more documentation, whether as part of the official aimet-ml docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/aimet-tech/aimet-ml/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>aimet-ml</code> for local development.</p> <ol> <li>Fork the <code>aimet-ml</code> repo on GitHub.</li> <li> <p>Clone your fork locally</p> <pre><code>$ git clone git@github.com:your_name_here/aimet-ml.git\n</code></pre> </li> <li> <p>Ensure poetry is installed.</p> </li> <li> <p>Install dependencies and start your virtualenv:</p> <pre><code>$ poetry install -E test -E doc -E dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</p> <pre><code>$ poetry run tox\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.9. Check    https://github.com/aimet-tech/aimet-ml/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#tips","title":"Tips","text":"<pre><code>$ poetry run pytest tests/test_aimet_ml.py\n</code></pre> <p>To run a subset of tests.</p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run:</p> <pre><code>$ poetry run bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre> <p>GitHub Actions will then deploy to PyPI if tests pass.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install aimet-ml, run this command in your terminal:</p> <pre><code>$ pip install aimet-ml\n</code></pre> <p>This is the preferred method to install aimet-ml, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source for aimet-ml can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>$ git clone git://github.com/aimet-tech/aimet-ml\n</code></pre> <p>Or download the tarball:</p> <pre><code>$ curl -OJL https://github.com/aimet-tech/aimet-ml/tarball/main\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>$ pip install .\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#usage","title":"Usage","text":"<p>To use aimet-ml in a project</p> <pre><code>import aimet_ml\n</code></pre>"},{"location":"api/features/","title":"features","text":""},{"location":"api/features/#aimet_ml.features.textual","title":"<code>textual</code>","text":""},{"location":"api/features/#aimet_ml.features.textual.transformers","title":"<code>transformers</code>","text":""},{"location":"api/features/#aimet_ml.features.textual.transformers.TransformerFeatureExtractor","title":"<code>TransformerFeatureExtractor</code>","text":"<p>Extracts features from input texts using transformer embeddings.</p> Source code in <code>aimet_ml/features/textual/transformers.py</code> <pre><code>class TransformerFeatureExtractor:\n    \"\"\"Extracts features from input texts using transformer embeddings.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        num_emb_layers: int = 4,\n        max_length: int = 512,\n        device: Union[str, torch.device] = \"cuda:0\",\n    ):\n        \"\"\"\n        Initializes the TransformerFeatureExtractor.\n\n        Args:\n            model_name (str): The name or path of the pre-trained transformer model.\n            num_emb_layers (int, optional): Number of layers to use for feature extraction. Default is 4.\n            max_length (int, optional): Maximum length of input text for tokenization. Default is 512.\n            device (str or torch.device, optional): Device to use for computation ('cuda:0', 'cpu', etc.).\n                Default is 'cuda:0' if available, else 'cpu'.\n        \"\"\"\n\n        if not torch.cuda.is_available():\n            device = \"cpu\"\n\n        self.model = AutoModel.from_pretrained(model_name)\n        self.model.to(device)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.num_emb_layers = num_emb_layers\n        self.max_length = max_length\n\n    def extract_features(self, texts: Union[str, List[str]]) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts features from input texts using transformer embeddings.\n\n        Args:\n            texts (str or list): Input text or list of texts for feature extraction.\n\n        Returns:\n            torch.Tensor: Extracted features for input texts.\n        \"\"\"\n        self.model.eval()\n\n        if isinstance(texts, str):\n            texts = [texts]\n\n        input_ids, attention_masks = [], []\n        for text in texts:\n            tokenized_output = self.tokenize(text)\n            input_ids.append(tokenized_output[\"input_ids\"])\n            attention_masks.append(tokenized_output[\"attention_mask\"])\n        input_ids_tensor = torch.cat(input_ids, dim=0).to(self.model.device)\n        attention_masks_tensor = torch.cat(attention_masks, dim=0).to(self.model.device)\n\n        with torch.no_grad():\n            hidden_states = self.model(\n                input_ids_tensor, attention_mask=attention_masks_tensor, output_hidden_states=True\n            )[\"hidden_states\"]\n\n        embeddings = sum(hidden_states[-i][:, 0, :] for i in range(1, self.num_emb_layers + 1))\n        embeddings = embeddings.detach().cpu()\n\n        return embeddings\n\n    def tokenize(self, text: str) -&gt; dict:\n        \"\"\"\n        Tokenizes input text using the transformer's tokenizer.\n\n        Args:\n            text (str): Input text to be tokenized.\n\n        Returns:\n            dict: Dictionary containing tokenized input with attention mask.\n        \"\"\"\n        tokenized_output = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        return tokenized_output.data\n</code></pre>"},{"location":"api/features/#aimet_ml.features.textual.transformers.TransformerFeatureExtractor.__init__","title":"<code>__init__(model_name, num_emb_layers=4, max_length=512, device='cuda:0')</code>","text":"<p>Initializes the TransformerFeatureExtractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name or path of the pre-trained transformer model.</p> required <code>num_emb_layers</code> <code>int</code> <p>Number of layers to use for feature extraction. Default is 4.</p> <code>4</code> <code>max_length</code> <code>int</code> <p>Maximum length of input text for tokenization. Default is 512.</p> <code>512</code> <code>device</code> <code>str or device</code> <p>Device to use for computation ('cuda:0', 'cpu', etc.). Default is 'cuda:0' if available, else 'cpu'.</p> <code>'cuda:0'</code> Source code in <code>aimet_ml/features/textual/transformers.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    num_emb_layers: int = 4,\n    max_length: int = 512,\n    device: Union[str, torch.device] = \"cuda:0\",\n):\n    \"\"\"\n    Initializes the TransformerFeatureExtractor.\n\n    Args:\n        model_name (str): The name or path of the pre-trained transformer model.\n        num_emb_layers (int, optional): Number of layers to use for feature extraction. Default is 4.\n        max_length (int, optional): Maximum length of input text for tokenization. Default is 512.\n        device (str or torch.device, optional): Device to use for computation ('cuda:0', 'cpu', etc.).\n            Default is 'cuda:0' if available, else 'cpu'.\n    \"\"\"\n\n    if not torch.cuda.is_available():\n        device = \"cpu\"\n\n    self.model = AutoModel.from_pretrained(model_name)\n    self.model.to(device)\n\n    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n    self.num_emb_layers = num_emb_layers\n    self.max_length = max_length\n</code></pre>"},{"location":"api/features/#aimet_ml.features.textual.transformers.TransformerFeatureExtractor.extract_features","title":"<code>extract_features(texts)</code>","text":"<p>Extracts features from input texts using transformer embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>str or list</code> <p>Input text or list of texts for feature extraction.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Extracted features for input texts.</p> Source code in <code>aimet_ml/features/textual/transformers.py</code> <pre><code>def extract_features(self, texts: Union[str, List[str]]) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts features from input texts using transformer embeddings.\n\n    Args:\n        texts (str or list): Input text or list of texts for feature extraction.\n\n    Returns:\n        torch.Tensor: Extracted features for input texts.\n    \"\"\"\n    self.model.eval()\n\n    if isinstance(texts, str):\n        texts = [texts]\n\n    input_ids, attention_masks = [], []\n    for text in texts:\n        tokenized_output = self.tokenize(text)\n        input_ids.append(tokenized_output[\"input_ids\"])\n        attention_masks.append(tokenized_output[\"attention_mask\"])\n    input_ids_tensor = torch.cat(input_ids, dim=0).to(self.model.device)\n    attention_masks_tensor = torch.cat(attention_masks, dim=0).to(self.model.device)\n\n    with torch.no_grad():\n        hidden_states = self.model(\n            input_ids_tensor, attention_mask=attention_masks_tensor, output_hidden_states=True\n        )[\"hidden_states\"]\n\n    embeddings = sum(hidden_states[-i][:, 0, :] for i in range(1, self.num_emb_layers + 1))\n    embeddings = embeddings.detach().cpu()\n\n    return embeddings\n</code></pre>"},{"location":"api/features/#aimet_ml.features.textual.transformers.TransformerFeatureExtractor.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenizes input text using the transformer's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to be tokenized.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing tokenized input with attention mask.</p> Source code in <code>aimet_ml/features/textual/transformers.py</code> <pre><code>def tokenize(self, text: str) -&gt; dict:\n    \"\"\"\n    Tokenizes input text using the transformer's tokenizer.\n\n    Args:\n        text (str): Input text to be tokenized.\n\n    Returns:\n        dict: Dictionary containing tokenized input with attention mask.\n    \"\"\"\n    tokenized_output = self.tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=self.max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors=\"pt\",\n    )\n\n    return tokenized_output.data\n</code></pre>"},{"location":"api/metrics/","title":"metrics","text":""},{"location":"api/metrics/#aimet_ml.metrics.plots","title":"<code>plots</code>","text":""},{"location":"api/metrics/#aimet_ml.metrics.plots.get_confusion_matrix","title":"<code>get_confusion_matrix(y_true, y_pred, display_labels, name='')</code>","text":"<p>Generate and display a confusion matrix plot.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Collection</code> <p>True labels.</p> required <code>y_pred</code> <code>Collection</code> <p>Predicted labels.</p> required <code>display_labels</code> <code>list</code> <p>Labels to be displayed on the confusion matrix plot.</p> required <code>name</code> <code>str</code> <p>Name of the plot. Defaults to an empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrixDisplay</code> <code>ConfusionMatrixDisplay</code> <p>A display object for the confusion matrix plot.</p> Source code in <code>aimet_ml/metrics/plots.py</code> <pre><code>def get_confusion_matrix(\n    y_true: Collection, y_pred: Collection, display_labels: list, name: str = \"\"\n) -&gt; ConfusionMatrixDisplay:\n    \"\"\"\n    Generate and display a confusion matrix plot.\n\n    Args:\n        y_true (Collection): True labels.\n        y_pred (Collection): Predicted labels.\n        display_labels (list): Labels to be displayed on the confusion matrix plot.\n        name (str, optional): Name of the plot. Defaults to an empty string.\n\n    Returns:\n        ConfusionMatrixDisplay: A display object for the confusion matrix plot.\n    \"\"\"\n    cm_display = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=display_labels)\n    cm_display.plot()\n    cm_display.ax_.set_title(name)\n    return cm_display\n</code></pre>"},{"location":"api/metrics/#aimet_ml.metrics.plots.get_prc_display","title":"<code>get_prc_display(y_true, y_score, name='')</code>","text":"<p>Generate and display a precision-recall curve plot.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Collection</code> <p>True labels.</p> required <code>y_score</code> <code>Collection</code> <p>Predicted scores or probabilities.</p> required <code>name</code> <code>str</code> <p>Name of the plot. Defaults to an empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>PrecisionRecallDisplay</code> <code>PrecisionRecallDisplay</code> <p>A display object for the precision-recall curve plot.</p> Source code in <code>aimet_ml/metrics/plots.py</code> <pre><code>def get_prc_display(y_true: Collection, y_score: Collection, name: str = \"\") -&gt; PrecisionRecallDisplay:\n    \"\"\"\n    Generate and display a precision-recall curve plot.\n\n    Args:\n        y_true (Collection): True labels.\n        y_score (Collection): Predicted scores or probabilities.\n        name (str, optional): Name of the plot. Defaults to an empty string.\n\n    Returns:\n        PrecisionRecallDisplay: A display object for the precision-recall curve plot.\n    \"\"\"\n    pr_display = PrecisionRecallDisplay.from_predictions(y_true, y_score, name=name)\n    pr_display.ax_.set_title(name)\n    return pr_display\n</code></pre>"},{"location":"api/metrics/#aimet_ml.metrics.plots.get_roc_display","title":"<code>get_roc_display(y_true, y_score, name='')</code>","text":"<p>Generate and display a ROC curve plot.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Collection</code> <p>True labels.</p> required <code>y_score</code> <code>Collection</code> <p>Predicted scores or probabilities.</p> required <code>name</code> <code>str</code> <p>Name of the plot. Defaults to an empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>RocCurveDisplay</code> <code>RocCurveDisplay</code> <p>A display object for the ROC curve plot.</p> Source code in <code>aimet_ml/metrics/plots.py</code> <pre><code>def get_roc_display(y_true: Collection, y_score: Collection, name: str = \"\") -&gt; RocCurveDisplay:\n    \"\"\"\n    Generate and display a ROC curve plot.\n\n    Args:\n        y_true (Collection): True labels.\n        y_score (Collection): Predicted scores or probabilities.\n        name (str, optional): Name of the plot. Defaults to an empty string.\n\n    Returns:\n        RocCurveDisplay: A display object for the ROC curve plot.\n    \"\"\"\n    roc_display = RocCurveDisplay.from_predictions(y_true, y_score, name=name)\n    roc_display.ax_.set_title(name)\n    return roc_display\n</code></pre>"},{"location":"api/metrics/#aimet_ml.metrics.reports","title":"<code>reports</code>","text":""},{"location":"api/metrics/#aimet_ml.metrics.reports.add_metric_to_report","title":"<code>add_metric_to_report(cls_report, metric_name, label_names, metric_values)</code>","text":"<p>Adds metric values to a classification report.</p> <p>Parameters:</p> Name Type Description Default <code>cls_report</code> <code>dict</code> <p>The classification report as a dictionary.</p> required <code>metric_name</code> <code>str</code> <p>The name of the metric to add.</p> required <code>label_names</code> <code>Collection[str]</code> <p>Collection of label names.</p> required <code>metric_values</code> <code>Collection[Union[float, int]]</code> <p>Collection of metric values corresponding to label_names.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the lengths of label_names and metric_values do not match.</p> Source code in <code>aimet_ml/metrics/reports.py</code> <pre><code>def add_metric_to_report(\n    cls_report: Dict[str, Dict[str, Any]],\n    metric_name: str,\n    label_names: Collection[str],\n    metric_values: Collection[Union[float, int]],\n) -&gt; None:\n    \"\"\"\n    Adds metric values to a classification report.\n\n    Args:\n        cls_report (dict): The classification report as a dictionary.\n        metric_name (str): The name of the metric to add.\n        label_names (Collection[str]): Collection of label names.\n        metric_values (Collection[Union[float, int]]): Collection of metric values corresponding to label_names.\n\n    Raises:\n        AssertionError: If the lengths of label_names and metric_values do not match.\n    \"\"\"\n    if len(label_names) != len(metric_values):\n        raise ValueError('label_names and metric_values must have the same length')\n\n    if len(set(label_names)) != len(label_names):\n        raise ValueError('Elements in label_names must be distinct from each other')\n\n    if any(label_name not in cls_report for label_name in label_names):\n        raise ValueError('All elements in label_names must be in the target_names of cls_report')\n\n    cls_report[\"macro avg\"][metric_name] = 0\n    cls_report[\"weighted avg\"][metric_name] = 0\n\n    for label_name, metric_value in zip(label_names, metric_values):\n        macro_w = 1 / len(label_names)\n        micro_w = cls_report[label_name][\"support\"] / cls_report[\"weighted avg\"][\"support\"]\n\n        cls_report[label_name][metric_name] = metric_value\n        cls_report[\"macro avg\"][metric_name] += metric_value * macro_w\n        cls_report[\"weighted avg\"][metric_name] += metric_value * micro_w\n</code></pre>"},{"location":"api/metrics/#aimet_ml.metrics.reports.flatten_dict","title":"<code>flatten_dict(d, prefix='')</code>","text":"<p>Recursively flattens a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>The input dictionary to flatten.</p> required <code>prefix</code> <code>str</code> <p>The prefix to be added to flattened keys. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>A flattened dictionary.</p> Source code in <code>aimet_ml/metrics/reports.py</code> <pre><code>def flatten_dict(d: Dict[str, Any], prefix: str = \"\") -&gt; Dict[str, Any]:\n    \"\"\"\n    Recursively flattens a nested dictionary.\n\n    Args:\n        d (dict): The input dictionary to flatten.\n        prefix (str, optional): The prefix to be added to flattened keys. Defaults to \"\".\n\n    Returns:\n        dict: A flattened dictionary.\n    \"\"\"\n    flat_dict = {}\n    for key, value in d.items():\n        new_key = f\"{prefix}_{key}\" if prefix else key\n        if isinstance(value, dict):\n            flat_dict.update(flatten_dict(value, prefix=new_key))\n        else:\n            flat_dict[new_key] = value\n    return flat_dict\n</code></pre>"},{"location":"api/model_selection/","title":"model_selection","text":""},{"location":"api/model_selection/#aimet_ml.model_selection.splits","title":"<code>splits</code>","text":""},{"location":"api/model_selection/#aimet_ml.model_selection.splits.get_splitter","title":"<code>get_splitter(stratify_cols=None, group_cols=None, n_splits=5, random_state=1414)</code>","text":"<p>Get a cross-validation splitter based on input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>stratify_cols</code> <code>Collection[str]</code> <p>Column names for stratification. Defaults to None.</p> <code>None</code> <code>group_cols</code> <code>Collection[str]</code> <p>Column names for grouping. Defaults to None.</p> <code>None</code> <code>n_splits</code> <code>int</code> <p>Number of splits in the cross-validation. Defaults to 5.</p> <code>5</code> <code>random_state</code> <code>int</code> <p>Seed for random number generator. Defaults to 1414.</p> <code>1414</code> <p>Returns:</p> Name Type Description <code>BaseCrossValidator</code> <code>BaseCrossValidator</code> <p>A cross-validation splitter based on the input parameters.</p> Source code in <code>aimet_ml/model_selection/splits.py</code> <pre><code>def get_splitter(\n    stratify_cols: Optional[Collection[str]] = None,\n    group_cols: Optional[Collection[str]] = None,\n    n_splits: int = 5,\n    random_state: int = 1414,\n) -&gt; BaseCrossValidator:\n    \"\"\"\n    Get a cross-validation splitter based on input parameters.\n\n    Args:\n        stratify_cols (Collection[str], optional): Column names for stratification. Defaults to None.\n        group_cols (Collection[str], optional): Column names for grouping. Defaults to None.\n        n_splits (int, optional): Number of splits in the cross-validation. Defaults to 5.\n        random_state (int): Seed for random number generator. Defaults to 1414.\n\n    Returns:\n        BaseCrossValidator: A cross-validation splitter based on the input parameters.\n    \"\"\"\n    if n_splits &lt;= 1:\n        raise ValueError(\"n_splits must be greater than 1\")\n\n    stratify_cols = stratify_cols if stratify_cols else None\n    group_cols = group_cols if group_cols else None\n\n    unique_stratify_cols = set(stratify_cols) if stratify_cols else set()\n    unique_group_cols = set(group_cols) if group_cols else set()\n\n    if unique_stratify_cols.intersection(unique_group_cols):\n        raise ValueError(\"group_cols and stratify_cols must be disjoint\")\n\n    if (stratify_cols is not None) and (group_cols is not None):\n        return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    if stratify_cols is not None:\n        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    if group_cols is not None:\n        return GroupKFold(n_splits=n_splits)\n\n    return KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n</code></pre>"},{"location":"api/model_selection/#aimet_ml.model_selection.splits.join_cols","title":"<code>join_cols(df, cols, sep='_')</code>","text":"<p>Concatenate the specified columns of a DataFrame with a separator.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to operate on.</p> required <code>cols</code> <code>Collection[str]</code> <p>Column names to concatenate.</p> required <code>sep</code> <code>str</code> <p>The separator to use between the column values. Defaults to \"_\".</p> <code>'_'</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A Series containing the concatenated values.</p> Source code in <code>aimet_ml/model_selection/splits.py</code> <pre><code>def join_cols(df: pd.DataFrame, cols: Collection[str], sep: str = \"_\") -&gt; pd.Series:\n    \"\"\"\n    Concatenate the specified columns of a DataFrame with a separator.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to operate on.\n        cols (Collection[str]): Column names to concatenate.\n        sep (str, optional): The separator to use between the column values. Defaults to \"_\".\n\n    Returns:\n        pd.Series: A Series containing the concatenated values.\n    \"\"\"\n    if len(cols) == 0:\n        raise ValueError(\"At least a column name is required, got empthy\")\n    return df[cols].apply(lambda row: sep.join(row.astype(str)), axis=1)\n</code></pre>"},{"location":"api/model_selection/#aimet_ml.model_selection.splits.split_dataset","title":"<code>split_dataset(dataset_df, test_fraction=0.2, val_n_splits=5, stratify_cols=None, group_cols=None, test_split_name='test', dev_split_name='dev', train_split_name_format='train_fold_{}', val_split_name_format='val_fold_{}', random_seed=1414)</code>","text":"<p>Split a dataset into development, test, and cross-validation sets with stratification and grouping.</p> <p>The dataset will be split into a development set and a test set. The development set will then be further split into k-fold cross-validation sets, each containing its own training and validation sets. The final data splits include a test set, k training sets, and k validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_df</code> <code>DataFrame</code> <p>The input DataFrame to be split.</p> required <code>test_fraction</code> <code>Union[float, int]</code> <p>The fraction of data to be used for testing.                                          If a float is given, it's rounded to the nearest fraction.                                          If an integer (n) is given, the fraction is calculated as 1/n.                                          Defaults to 0.2.</p> <code>0.2</code> <code>val_n_splits</code> <code>int</code> <p>Number of cross-validation splits. Defaults to 5.</p> <code>5</code> <code>stratify_cols</code> <code>Collection[str]</code> <p>Column names for stratification. Defaults to None.</p> <code>None</code> <code>group_cols</code> <code>Collection[str]</code> <p>Column names for grouping. Defaults to None.</p> <code>None</code> <code>test_split_name</code> <code>str</code> <p>Name for the test split. Defaults to \"test\".</p> <code>'test'</code> <code>dev_split_name</code> <code>str</code> <p>Name for the development split. Defaults to \"dev\".</p> <code>'dev'</code> <code>train_split_name_format</code> <code>str</code> <p>Format for naming training splits. Defaults to \"train_fold_{}\".</p> <code>'train_fold_{}'</code> <code>val_split_name_format</code> <code>str</code> <p>Format for naming validation splits. Defaults to \"val_fold_{}\".</p> <code>'val_fold_{}'</code> <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 1414.</p> <code>1414</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dict[str, pd.DataFrame]: A dictionary containing the split DataFrames.</p> Source code in <code>aimet_ml/model_selection/splits.py</code> <pre><code>def split_dataset(\n    dataset_df: pd.DataFrame,\n    test_fraction: Union[float, int] = 0.2,\n    val_n_splits: int = 5,\n    stratify_cols: Optional[Collection[str]] = None,\n    group_cols: Optional[Collection[str]] = None,\n    test_split_name: str = \"test\",\n    dev_split_name: str = \"dev\",\n    train_split_name_format: str = \"train_fold_{}\",\n    val_split_name_format: str = \"val_fold_{}\",\n    random_seed: int = 1414,\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Split a dataset into development, test, and cross-validation sets with stratification and grouping.\n\n    The dataset will be split into a development set and a test set. The development set will then be further\n    split into k-fold cross-validation sets, each containing its own training and validation sets.\n    The final data splits include a test set, k training sets, and k validation sets.\n\n    Args:\n        dataset_df (pd.DataFrame): The input DataFrame to be split.\n        test_fraction (Union[float, int], optional): The fraction of data to be used for testing.\n                                                     If a float is given, it's rounded to the nearest fraction.\n                                                     If an integer (n) is given, the fraction is calculated as 1/n.\n                                                     Defaults to 0.2.\n        val_n_splits (int, optional): Number of cross-validation splits. Defaults to 5.\n        stratify_cols (Collection[str], optional): Column names for stratification. Defaults to None.\n        group_cols (Collection[str], optional): Column names for grouping. Defaults to None.\n        test_split_name (str, optional): Name for the test split. Defaults to \"test\".\n        dev_split_name (str, optional): Name for the development split. Defaults to \"dev\".\n        train_split_name_format (str, optional): Format for naming training splits. Defaults to \"train_fold_{}\".\n        val_split_name_format (str, optional): Format for naming validation splits. Defaults to \"val_fold_{}\".\n        random_seed (int, optional): Random seed for reproducibility. Defaults to 1414.\n\n    Returns:\n        Dict[str, pd.DataFrame]: A dictionary containing the split DataFrames.\n    \"\"\"\n    if val_n_splits &lt;= 1:\n        raise ValueError(\"val_n_splits must be greater than 1\")\n\n    data_splits = dict()\n\n    # split into dev and test datasets\n    dev_dataset_df, test_dataset_df = stratified_group_split(\n        dataset_df, test_fraction, stratify_cols, group_cols, random_seed\n    )\n    data_splits[dev_split_name] = dev_dataset_df\n    data_splits[test_split_name] = test_dataset_df\n\n    # cross-validation split\n    k_fold_splitter = get_splitter(stratify_cols, group_cols, val_n_splits, random_seed)\n\n    dev_stratify = join_cols(dev_dataset_df, stratify_cols) if stratify_cols else None\n    dev_groups = join_cols(dev_dataset_df, group_cols) if group_cols else None\n\n    for n, (train_rows, val_rows) in enumerate(\n        k_fold_splitter.split(X=dev_dataset_df, y=dev_stratify, groups=dev_groups)\n    ):\n        k = n + 1\n        data_splits[train_split_name_format.format(k)] = dev_dataset_df.iloc[train_rows].reset_index(drop=True)\n        data_splits[val_split_name_format.format(k)] = dev_dataset_df.iloc[val_rows].reset_index(drop=True)\n\n    return data_splits\n</code></pre>"},{"location":"api/model_selection/#aimet_ml.model_selection.splits.split_dataset_v2","title":"<code>split_dataset_v2(dataset_df, val_fraction=0.1, test_n_splits=5, stratify_cols=None, group_cols=None, train_split_name_format='train_fold_{}', val_split_name_format='val_fold_{}', test_split_name_format='test_fold_{}', random_seed=1414)</code>","text":"<p>Split a dataset into k-fold cross-validation sets with stratification and grouping.</p> <p>The dataset will be split into k-fold cross-validation sets, each containing development and test sets. For each fold, the development set will be further split into training and validation sets. The final data splits include k test sets, k training sets, and k validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_df</code> <code>DataFrame</code> <p>The input DataFrame to be split.</p> required <code>val_fraction</code> <code>Union[float, int]</code> <p>The fraction of data to be used for validation.                                          If a float is given, it's rounded to the nearest fraction.                                          If an integer (n) is given, the fraction is calculated as 1/n.                                          Defaults to 0.1.</p> <code>0.1</code> <code>test_n_splits</code> <code>int</code> <p>Number of cross-validation splits. Defaults to 5.</p> <code>5</code> <code>stratify_cols</code> <code>Collection[str]</code> <p>Column names for stratification. Defaults to None.</p> <code>None</code> <code>group_cols</code> <code>Collection[str]</code> <p>Column names for grouping. Defaults to None.</p> <code>None</code> <code>train_split_name_format</code> <code>str</code> <p>Format for naming training splits. Defaults to \"train_fold_{}\".</p> <code>'train_fold_{}'</code> <code>val_split_name_format</code> <code>str</code> <p>Format for naming validation splits. Defaults to \"val_fold_{}\".</p> <code>'val_fold_{}'</code> <code>test_split_name_format</code> <code>str</code> <p>Format for naming validation splits. Defaults to \"test_fold_{}\".</p> <code>'test_fold_{}'</code> <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 1414.</p> <code>1414</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>Dict[str, pd.DataFrame]: A dictionary containing the split DataFrames.</p> Source code in <code>aimet_ml/model_selection/splits.py</code> <pre><code>def split_dataset_v2(\n    dataset_df: pd.DataFrame,\n    val_fraction: Union[float, int] = 0.1,\n    test_n_splits: int = 5,\n    stratify_cols: Optional[Collection[str]] = None,\n    group_cols: Optional[Collection[str]] = None,\n    train_split_name_format: str = \"train_fold_{}\",\n    val_split_name_format: str = \"val_fold_{}\",\n    test_split_name_format: str = \"test_fold_{}\",\n    random_seed: int = 1414,\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Split a dataset into k-fold cross-validation sets with stratification and grouping.\n\n    The dataset will be split into k-fold cross-validation sets, each containing development and test sets.\n    For each fold, the development set will be further split into training and validation sets.\n    The final data splits include k test sets, k training sets, and k validation sets.\n\n    Args:\n        dataset_df (pd.DataFrame): The input DataFrame to be split.\n        val_fraction (Union[float, int], optional): The fraction of data to be used for validation.\n                                                     If a float is given, it's rounded to the nearest fraction.\n                                                     If an integer (n) is given, the fraction is calculated as 1/n.\n                                                     Defaults to 0.1.\n        test_n_splits (int, optional): Number of cross-validation splits. Defaults to 5.\n        stratify_cols (Collection[str], optional): Column names for stratification. Defaults to None.\n        group_cols (Collection[str], optional): Column names for grouping. Defaults to None.\n        train_split_name_format (str, optional): Format for naming training splits. Defaults to \"train_fold_{}\".\n        val_split_name_format (str, optional): Format for naming validation splits. Defaults to \"val_fold_{}\".\n        test_split_name_format (str, optional): Format for naming validation splits. Defaults to \"test_fold_{}\".\n        random_seed (int, optional): Random seed for reproducibility. Defaults to 1414.\n\n    Returns:\n        Dict[str, pd.DataFrame]: A dictionary containing the split DataFrames.\n    \"\"\"\n    if test_n_splits &lt;= 1:\n        raise ValueError(\"test_n_splits must be greater than 1\")\n\n    data_splits = dict()\n\n    # cross-validation split\n    k_fold_splitter = get_splitter(stratify_cols, group_cols, test_n_splits, random_seed)\n\n    stratify = join_cols(dataset_df, stratify_cols) if stratify_cols else None\n    groups = join_cols(dataset_df, group_cols) if group_cols else None\n\n    for n, (dev_rows, test_rows) in enumerate(k_fold_splitter.split(X=dataset_df, y=stratify, groups=groups)):\n        k = n + 1\n        data_splits[test_split_name_format.format(k)] = dataset_df.iloc[test_rows].reset_index(drop=True)\n\n        # split into training and validation sets\n        dev_dataset_df = dataset_df.iloc[dev_rows].reset_index(drop=True)\n        train_dataset_df, val_dataset_df = stratified_group_split(\n            dev_dataset_df, val_fraction, stratify_cols, group_cols, random_seed\n        )\n        data_splits[train_split_name_format.format(k)] = train_dataset_df\n        data_splits[val_split_name_format.format(k)] = val_dataset_df\n\n    return data_splits\n</code></pre>"},{"location":"api/model_selection/#aimet_ml.model_selection.splits.stratified_group_split","title":"<code>stratified_group_split(dataset_df, test_fraction=0.2, stratify_cols=None, group_cols=None, random_seed=1414)</code>","text":"<p>Split a dataset into development and test sets with stratification and grouping.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_df</code> <code>DataFrame</code> <p>The input DataFrame to be split.</p> required <code>test_fraction</code> <code>Union[float, int]</code> <p>The fraction of data to be used for testing.                                          If a float (0, 1) is given, it's rounded to the nearest fraction.                                          If an integer (n &gt; 1) is given, the fraction is calculated as 1/n.                                          Defaults to 0.2.</p> <code>0.2</code> <code>stratify_cols</code> <code>Collection[str]</code> <p>Column names for stratification. Defaults to None.</p> <code>None</code> <code>group_cols</code> <code>Collection[str]</code> <p>Column names for grouping. Defaults to None.</p> <code>None</code> <code>random_seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 1414.</p> <code>1414</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the development and test DataFrames.</p> Source code in <code>aimet_ml/model_selection/splits.py</code> <pre><code>def stratified_group_split(\n    dataset_df: pd.DataFrame,\n    test_fraction: Union[float, int] = 0.2,\n    stratify_cols: Optional[Collection[str]] = None,\n    group_cols: Optional[Collection[str]] = None,\n    random_seed: int = 1414,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Split a dataset into development and test sets with stratification and grouping.\n\n    Args:\n        dataset_df (pd.DataFrame): The input DataFrame to be split.\n        test_fraction (Union[float, int], optional): The fraction of data to be used for testing.\n                                                     If a float (0, 1) is given, it's rounded to the nearest fraction.\n                                                     If an integer (n &gt; 1) is given, the fraction is calculated as 1/n.\n                                                     Defaults to 0.2.\n        stratify_cols (Collection[str], optional): Column names for stratification. Defaults to None.\n        group_cols (Collection[str], optional): Column names for grouping. Defaults to None.\n        random_seed (int, optional): Random seed for reproducibility. Defaults to 1414.\n\n    Returns:\n        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the development and test DataFrames.\n    \"\"\"\n    if test_fraction &lt;= 0:\n        raise ValueError(\"test_fraction must be greater than 0\")\n\n    if test_fraction == 1:\n        raise ValueError(\"test_fraction must not equal to 1\")\n\n    if isinstance(test_fraction, float) and (test_fraction &gt; 1):\n        raise ValueError(\"test_fraction provided as float must be less than 1\")\n\n    if isinstance(test_fraction, int):\n        test_fraction = 1 / test_fraction\n\n    split_fraction = min(test_fraction, 1 - test_fraction)\n    n_splits = round(1 / split_fraction)\n    splitter = get_splitter(stratify_cols, group_cols, n_splits, random_seed)\n\n    stratify = join_cols(dataset_df, stratify_cols) if stratify_cols else None\n    groups = join_cols(dataset_df, group_cols) if group_cols else None\n\n    lowest_diff = float('inf')\n    best_dev_rows, best_test_rows = None, None\n    for dev_rows, test_rows in splitter.split(X=dataset_df, y=stratify, groups=groups):\n        fraction = len(test_rows) / len(dataset_df)\n        diff = abs(split_fraction - fraction)\n        if diff &lt; lowest_diff:\n            lowest_diff = diff\n            best_dev_rows = dev_rows\n            best_test_rows = test_rows\n\n    if test_fraction &gt; 0.5:\n        best_dev_rows, best_test_rows = best_test_rows, best_dev_rows\n\n    dev_dataset_df = dataset_df.iloc[best_dev_rows].reset_index(drop=True)\n    test_dataset_df = dataset_df.iloc[best_test_rows].reset_index(drop=True)\n\n    return dev_dataset_df, test_dataset_df\n</code></pre>"},{"location":"api/processing/","title":"processing","text":""},{"location":"api/processing/#aimet_ml.processing.audio","title":"<code>audio</code>","text":""},{"location":"api/processing/#aimet_ml.processing.audio.convert_audio","title":"<code>convert_audio(src_file, dst_file, target_sr=None, normalize=False)</code>","text":"<p>Convert an audio file to a different sample rate and save it to a new file.</p> <p>Parameters:</p> Name Type Description Default <code>src_file</code> <code>str</code> <p>Path to the source audio file.</p> required <code>dst_file</code> <code>str</code> <p>Path to the destination audio file.</p> required <code>target_sr</code> <code>int</code> <p>Target sample rate for the output audio file.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the audio waveform before conversion.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>AudioSegment</code> <code>AudioSegment</code> <p>The converted audio waveform as an AudioSegment object.</p> Source code in <code>aimet_ml/processing/audio.py</code> <pre><code>def convert_audio(\n    src_file: str, dst_file: str, target_sr: Optional[int] = None, normalize: bool = False\n) -&gt; AudioSegment:\n    \"\"\"\n    Convert an audio file to a different sample rate and save it to a new file.\n\n    Args:\n        src_file (str): Path to the source audio file.\n        dst_file (str): Path to the destination audio file.\n        target_sr (int, optional): Target sample rate for the output audio file.\n        normalize (bool, optional): If True, normalize the audio waveform before conversion.\n\n    Returns:\n        AudioSegment: The converted audio waveform as an AudioSegment object.\n    \"\"\"\n    audio = read_audio(src_file, target_sr, normalize)\n    output_format = dst_file.split(\".\")[-1]\n    audio.export(dst_file, format=output_format)\n    return audio\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.audio.load_audio","title":"<code>load_audio(file_path, target_sr=None, normalize=False)</code>","text":"<p>Load an audio file and return the waveform as a NumPy array and the target sample rate.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file.</p> required <code>target_sr</code> <code>int</code> <p>Target sample rate for the audio waveform.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the audio waveform.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, int]</code> <p>Tuple[np.ndarray, int]: A tuple containing the waveform as a NumPy array and the target sample rate.</p> Source code in <code>aimet_ml/processing/audio.py</code> <pre><code>def load_audio(file_path: str, target_sr: Optional[int] = None, normalize: bool = False) -&gt; Tuple[np.ndarray, int]:\n    \"\"\"\n    Load an audio file and return the waveform as a NumPy array and the target sample rate.\n\n    Args:\n        file_path (str): Path to the audio file.\n        target_sr (int, optional): Target sample rate for the audio waveform.\n        normalize (bool, optional): If True, normalize the audio waveform.\n\n    Returns:\n        Tuple[np.ndarray, int]: A tuple containing the waveform as a NumPy array and the target sample rate.\n    \"\"\"\n    audio = read_audio(file_path, target_sr, normalize)\n    waveform = np.asarray(audio.get_array_of_samples(), dtype=np.float32) / 32768.0\n    sample_rate = audio.frame_rate\n    return waveform, sample_rate\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.audio.read_audio","title":"<code>read_audio(file_path, target_sr=None, normalize=False)</code>","text":"<p>Read an audio file and return the waveform as an AudioSegment object with the target sample rate.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the audio file.</p> required <code>target_sr</code> <code>int</code> <p>Target sample rate for the audio waveform.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the audio waveform.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>AudioSegment</code> <code>AudioSegment</code> <p>Audio waveform as an AudioSegment object.</p> Source code in <code>aimet_ml/processing/audio.py</code> <pre><code>def read_audio(file_path: str, target_sr: Optional[int] = None, normalize: bool = False) -&gt; AudioSegment:\n    \"\"\"\n    Read an audio file and return the waveform as an AudioSegment object with the target sample rate.\n\n    Args:\n        file_path (str): Path to the audio file.\n        target_sr (int, optional): Target sample rate for the audio waveform.\n        normalize (bool, optional): If True, normalize the audio waveform.\n\n    Returns:\n        AudioSegment: Audio waveform as an AudioSegment object.\n    \"\"\"\n    audio = AudioSegment.from_file(file_path)\n    if target_sr:\n        audio = audio.set_frame_rate(target_sr)\n    if normalize:\n        audio = effects.normalize(audio)\n    return audio\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.text","title":"<code>text</code>","text":""},{"location":"api/processing/#aimet_ml.processing.text.clean_repeated_tokens","title":"<code>clean_repeated_tokens(tokens)</code>","text":"<p>Remove sequences of repeated tokens from a list.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[str]</code> <p>List of tokens to clean.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>list[str]: List of tokens with repeated sequences removed.</p> Source code in <code>aimet_ml/processing/text.py</code> <pre><code>def clean_repeated_tokens(tokens: List[str]) -&gt; List[str]:\n    \"\"\"\n    Remove sequences of repeated tokens from a list.\n\n    Args:\n        tokens (list[str]): List of tokens to clean.\n\n    Returns:\n        list[str]: List of tokens with repeated sequences removed.\n    \"\"\"\n    tokens = tokens.copy()\n    sequence_size = len(tokens) // 2\n    while sequence_size &gt; 0:\n        cur_idx = 0\n        while cur_idx &lt; len(tokens) - sequence_size:\n            next_idx = cur_idx + sequence_size\n            cur_text = \"\".join(tokens[cur_idx : cur_idx + sequence_size])\n            next_text = \"\".join(tokens[next_idx : next_idx + sequence_size])\n            if cur_text == next_text:\n                tokens = tokens[: cur_idx + sequence_size] + tokens[next_idx + sequence_size :]\n            else:\n                cur_idx += 1\n        sequence_size -= 1\n    return tokens\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.text.exclude_keywords","title":"<code>exclude_keywords(text, keywords)</code>","text":"<p>Check if any of the given keywords are present in the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to search for keywords.</p> required <code>keywords</code> <code>list[str]</code> <p>List of keywords to check for.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>False if any keyword is present in the text, True otherwise.</p> Source code in <code>aimet_ml/processing/text.py</code> <pre><code>def exclude_keywords(text: str, keywords: List[str]) -&gt; bool:\n    \"\"\"\n    Check if any of the given keywords are present in the text.\n\n    Args:\n        text (str): The text to search for keywords.\n        keywords (list[str]): List of keywords to check for.\n\n    Returns:\n        bool: False if any keyword is present in the text, True otherwise.\n    \"\"\"\n    for kw in keywords:\n        if kw in text:\n            return False\n    return True\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.text.include_keywords","title":"<code>include_keywords(text, keywords)</code>","text":"<p>Check if any of the given keywords are present in the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to search for keywords.</p> required <code>keywords</code> <code>list[str]</code> <p>List of keywords to check for.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if any keyword is present in the text, False otherwise.</p> Source code in <code>aimet_ml/processing/text.py</code> <pre><code>def include_keywords(text: str, keywords: List[str]) -&gt; bool:\n    \"\"\"\n    Check if any of the given keywords are present in the text.\n\n    Args:\n        text (str): The text to search for keywords.\n        keywords (list[str]): List of keywords to check for.\n\n    Returns:\n        bool: True if any keyword is present in the text, False otherwise.\n    \"\"\"\n    for kw in keywords:\n        if kw in text:\n            return True\n    return False\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.text.trim_tokens","title":"<code>trim_tokens(tokenizer, text, max_len)</code>","text":"<p>Trims a list of tokens generated by a tokenizer to ensure it doesn't exceed a maximum length.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer used to tokenize the input text.</p> required <code>text</code> <code>str</code> <p>The input text to tokenize and trim.</p> required <code>max_len</code> <code>int</code> <p>The maximum allowed length for the list of tokens.</p> required <p>Returns:</p> Type Description <code>Tuple[str, int]</code> <p>Tuple[str, int]: A tuple containing the trimmed text and the number of tokens in the trimmed list.</p> Source code in <code>aimet_ml/processing/text.py</code> <pre><code>def trim_tokens(tokenizer: PreTrainedTokenizer, text: str, max_len: int) -&gt; Tuple[str, int]:\n    \"\"\"\n    Trims a list of tokens generated by a tokenizer to ensure it doesn't exceed a maximum length.\n\n    Args:\n        tokenizer (PreTrainedTokenizer): The tokenizer used to tokenize the input text.\n        text (str): The input text to tokenize and trim.\n        max_len (int): The maximum allowed length for the list of tokens.\n\n    Returns:\n        Tuple[str, int]: A tuple containing the trimmed text and the number of tokens in the trimmed list.\n    \"\"\"\n    tokens = tokenizer.tokenize(text)\n\n    if len(tokens) &gt; max_len:\n        tokens = tokens[:max_len]\n\n    return tokenizer.convert_tokens_to_string(tokens), len(tokens)\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.video","title":"<code>video</code>","text":""},{"location":"api/processing/#aimet_ml.processing.video.convert_video","title":"<code>convert_video(src_file, dst_file, target_fps)</code>","text":"<p>Convert a video to a different frame rate and save to a new file.</p> <p>Parameters:</p> Name Type Description Default <code>src_file</code> <code>str</code> <p>Path to the source video file.</p> required <code>dst_file</code> <code>str</code> <p>Path to the output video file.</p> required <code>target_fps</code> <code>int</code> <p>The target frames per second for the output video.</p> required Source code in <code>aimet_ml/processing/video.py</code> <pre><code>def convert_video(src_file: str, dst_file: str, target_fps: int) -&gt; None:\n    \"\"\"\n    Convert a video to a different frame rate and save to a new file.\n\n    Args:\n        src_file (str): Path to the source video file.\n        dst_file (str): Path to the output video file.\n        target_fps (int): The target frames per second for the output video.\n    \"\"\"\n    input_vid = ffmpeg.input(src_file)\n\n    audio = input_vid.audio\n    video = input_vid.video.filter(\"fps\", target_fps)\n    (\n        ffmpeg.output(\n            video,\n            audio,\n            dst_file,\n            acodec=\"aac\",\n            loglevel=\"quiet\",\n            max_muxing_queue_size=1024,\n        )\n        .overwrite_output()\n        .run()\n    )\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.video.is_video","title":"<code>is_video(file_path)</code>","text":"<p>Check if a given file contains video streams.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the input file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file contains video streams, False otherwise.</p> Source code in <code>aimet_ml/processing/video.py</code> <pre><code>def is_video(file_path: str) -&gt; bool:\n    \"\"\"\n    Check if a given file contains video streams.\n\n    Args:\n        file_path (str): The path to the input file.\n\n    Returns:\n        bool: True if the file contains video streams, False otherwise.\n    \"\"\"\n    probe = ffmpeg.probe(file_path)\n    streams = probe[\"streams\"]\n\n    for stream in streams:\n        if stream[\"codec_type\"] == \"video\":\n            return True\n\n    return False\n</code></pre>"},{"location":"api/processing/#aimet_ml.processing.video.load_video","title":"<code>load_video(file_path)</code>","text":"<p>Load frames from a video file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the video file.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>A tuple containing a list of frames and the frames per second (fps).</p> Source code in <code>aimet_ml/processing/video.py</code> <pre><code>def load_video(file_path: str) -&gt; tuple:\n    \"\"\"\n    Load frames from a video file.\n\n    Args:\n        file_path (str): The path to the video file.\n\n    Returns:\n        tuple: A tuple containing a list of frames and the frames per second (fps).\n    \"\"\"\n    frames: List[np.ndarray] = []\n    cap = cv2.VideoCapture(file_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n\n    cap.release()\n    return frames, fps\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#aimet_ml.utils.aws","title":"<code>aws</code>","text":""},{"location":"api/utils/#aimet_ml.utils.aws.download_s3","title":"<code>download_s3(bucket_name, object_name, output_path)</code>","text":"<p>Download a file from an S3 bucket and save it to the local file system.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>object_name</code> <code>str</code> <p>The key or path of the object to be downloaded from the bucket.</p> required <code>output_path</code> <code>str</code> <p>The local file path to save the downloaded object.</p> required Source code in <code>aimet_ml/utils/aws.py</code> <pre><code>def download_s3(bucket_name: str, object_name: str, output_path: str):\n    \"\"\"\n    Download a file from an S3 bucket and save it to the local file system.\n\n    Args:\n        bucket_name (str): The name of the S3 bucket.\n        object_name (str): The key or path of the object to be downloaded from the bucket.\n        output_path (str): The local file path to save the downloaded object.\n    \"\"\"\n    s3 = boto3.client(\"s3\")\n    s3.download_file(bucket_name, object_name, output_path)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.aws.upload_dir_s3","title":"<code>upload_dir_s3(bucket_name, bucket_dir_path, src_dir_path)</code>","text":"<p>Upload a local directory to an S3 bucket, preserving the directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>bucket_dir_path</code> <code>str</code> <p>The path within the bucket where the local directory will be uploaded.</p> required <code>src_dir_path</code> <code>str</code> <p>The local directory path to be uploaded.</p> required Source code in <code>aimet_ml/utils/aws.py</code> <pre><code>def upload_dir_s3(bucket_name: str, bucket_dir_path: str, src_dir_path: str):\n    \"\"\"\n    Upload a local directory to an S3 bucket, preserving the directory structure.\n\n    Args:\n        bucket_name (str): The name of the S3 bucket.\n        bucket_dir_path (str): The path within the bucket where the local directory will be uploaded.\n        src_dir_path (str): The local directory path to be uploaded.\n    \"\"\"\n    s3 = boto3.client(\"s3\")\n    src_dir_name = os.path.basename(src_dir_path)\n    for r, _, f in tqdm(os.walk(src_dir_path)):\n        for n in f:\n            file_path = os.path.join(r, n)\n            relpath = os.path.relpath(r, src_dir_path)\n            object_name = os.path.join(bucket_dir_path, src_dir_name, \"\" if relpath == \".\" else relpath, n)\n            s3.upload_file(file_path, bucket_name, object_name)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.aws.upload_files_s3","title":"<code>upload_files_s3(bucket_name, bucket_dir_path, src_file_paths)</code>","text":"<p>Upload multiple local files to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>bucket_dir_path</code> <code>str</code> <p>The path within the bucket where the files will be uploaded.</p> required <code>src_file_paths</code> <code>list</code> <p>A list of local file paths to be uploaded to the bucket.</p> required Source code in <code>aimet_ml/utils/aws.py</code> <pre><code>def upload_files_s3(bucket_name: str, bucket_dir_path: str, src_file_paths: list):\n    \"\"\"\n    Upload multiple local files to an S3 bucket.\n\n    Args:\n        bucket_name (str): The name of the S3 bucket.\n        bucket_dir_path (str): The path within the bucket where the files will be uploaded.\n        src_file_paths (list): A list of local file paths to be uploaded to the bucket.\n    \"\"\"\n    s3 = boto3.client(\"s3\")\n    for file_path in tqdm(src_file_paths):\n        object_name = os.path.join(bucket_dir_path, os.path.basename(file_path))\n        s3.upload_file(file_path, bucket_name, object_name)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.git","title":"<code>git</code>","text":""},{"location":"api/utils/#aimet_ml.utils.git.get_commit_id","title":"<code>get_commit_id(short=True)</code>","text":"<p>Get the Git commit ID of the current repository.</p> <p>Parameters:</p> Name Type Description Default <code>short</code> <code>bool</code> <p>Whether to get a short or full Git commit ID. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Git commit ID as a string.</p> Source code in <code>aimet_ml/utils/git.py</code> <pre><code>def get_commit_id(short: bool = True) -&gt; str:\n    \"\"\"\n    Get the Git commit ID of the current repository.\n\n    Args:\n        short (bool, optional): Whether to get a short or full Git commit ID. Defaults to True.\n\n    Returns:\n        str: The Git commit ID as a string.\n    \"\"\"\n    if short:\n        git_command = \"git rev-parse --short HEAD\"\n    else:\n        git_command = \"git rev-parse HEAD\"\n\n    commit_id = os.popen(git_command).read().strip()\n\n    return commit_id\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.hamd_7","title":"<code>hamd_7</code>","text":""},{"location":"api/utils/#aimet_ml.utils.hamd_7.score_to_severity","title":"<code>score_to_severity(score)</code>","text":"<p>Convert a score to a severity category.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>int</code> <p>The input score.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The corresponding severity category (\"normal\", \"mild\", \"moderate\", or \"severe\").</p> Source code in <code>aimet_ml/utils/hamd_7.py</code> <pre><code>def score_to_severity(score: int) -&gt; str:\n    \"\"\"\n    Convert a score to a severity category.\n\n    Args:\n        score (int): The input score.\n\n    Returns:\n        str: The corresponding severity category (\"normal\", \"mild\", \"moderate\", or \"severe\").\n    \"\"\"\n    if score &lt; 5:\n        return \"normal\"\n    if score &lt; 13:\n        return \"mild\"\n    if score &lt; 21:\n        return \"moderate\"\n    return \"severe\"\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.io_utils","title":"<code>io_utils</code>","text":""},{"location":"api/utils/#aimet_ml.utils.io_utils.read_json","title":"<code>read_json(file_path)</code>","text":"<p>Read and parse a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the JSON file to be read.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the parsed JSON data.</p> Source code in <code>aimet_ml/utils/io_utils.py</code> <pre><code>def read_json(file_path: str) -&gt; dict:\n    \"\"\"\n    Read and parse a JSON file.\n\n    Args:\n        file_path (str): The path to the JSON file to be read.\n\n    Returns:\n        dict: A dictionary containing the parsed JSON data.\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        data = json.load(f)\n    return data\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.io_utils.read_pickle","title":"<code>read_pickle(file_path)</code>","text":"<p>Read and unpickle a binary pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the pickle file to be read.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>The unpickled object.</p> Source code in <code>aimet_ml/utils/io_utils.py</code> <pre><code>def read_pickle(file_path: str):\n    \"\"\"\n    Read and unpickle a binary pickle file.\n\n    Args:\n        file_path (str): The path to the pickle file to be read.\n\n    Returns:\n        Any: The unpickled object.\n    \"\"\"\n    with open(file_path, \"rb\") as f:\n        data = pickle.load(f)\n    return data\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.io_utils.read_yaml","title":"<code>read_yaml(file_path)</code>","text":"<p>Read and parse a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the YAML file to be read.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the parsed YAML data.</p> Source code in <code>aimet_ml/utils/io_utils.py</code> <pre><code>def read_yaml(file_path: str) -&gt; dict:\n    \"\"\"\n    Read and parse a YAML file.\n\n    Args:\n        file_path (str): The path to the YAML file to be read.\n\n    Returns:\n        dict: A dictionary containing the parsed YAML data.\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        result = yaml.safe_load(f)\n    return result\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.io_utils.write_json","title":"<code>write_json(file_path, data, indent=None)</code>","text":"<p>Write data to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the JSON file to be written.</p> required <code>data</code> <code>dict</code> <p>The data to be written to the JSON file.</p> required <code>indent</code> <code>int</code> <p>The number of spaces to use for indentation.</p> <code>None</code> Source code in <code>aimet_ml/utils/io_utils.py</code> <pre><code>def write_json(file_path: str, data: dict, indent: Optional[int] = None):\n    \"\"\"\n    Write data to a JSON file.\n\n    Args:\n        file_path (str): The path to the JSON file to be written.\n        data (dict): The data to be written to the JSON file.\n        indent (int, optional): The number of spaces to use for indentation.\n    \"\"\"\n    with open(file_path, \"w\") as f:\n        json.dump(data, f, indent=indent)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.io_utils.write_pickle","title":"<code>write_pickle(file_path, data)</code>","text":"<p>Write data to a binary pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the pickle file to be written.</p> required <code>data</code> <p>The data to be pickled and written to the file.</p> required Source code in <code>aimet_ml/utils/io_utils.py</code> <pre><code>def write_pickle(file_path: str, data):\n    \"\"\"\n    Write data to a binary pickle file.\n\n    Args:\n        file_path (str): The path to the pickle file to be written.\n        data: The data to be pickled and written to the file.\n    \"\"\"\n    with open(file_path, \"wb\") as f:\n        pickle.dump(data, f)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.io_utils.write_yaml","title":"<code>write_yaml(file_path, data, default_flow_style=False)</code>","text":"<p>Write data to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the YAML file to be written.</p> required <code>data</code> <code>dict</code> <p>The data to be written to the YAML file.</p> required <code>default_flow_style</code> <code>bool</code> <p>Whether to use the default flow style for YAML.</p> <code>False</code> Source code in <code>aimet_ml/utils/io_utils.py</code> <pre><code>def write_yaml(file_path: str, data: dict, default_flow_style: bool = False):\n    \"\"\"\n    Write data to a YAML file.\n\n    Args:\n        file_path (str): The path to the YAML file to be written.\n        data (dict): The data to be written to the YAML file.\n        default_flow_style (bool, optional): Whether to use the default flow style for YAML.\n    \"\"\"\n    with open(file_path, \"w\") as f:\n        yaml.dump(data, f, default_flow_style=default_flow_style)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.plots","title":"<code>plots</code>","text":""},{"location":"api/utils/#aimet_ml.utils.plots.add_bar_label","title":"<code>add_bar_label(bar_chart, with_percent=False, percent_digits=2)</code>","text":"<p>Add labels to a bar chart with optional percentage values.</p> <p>Parameters:</p> Name Type Description Default <code>bar_chart</code> <code>Axes</code> <p>The bar chart object.</p> required <code>with_percent</code> <code>bool</code> <p>Whether to include percentage values. Defaults to False.</p> <code>False</code> <code>percent_digits</code> <code>int</code> <p>Number of decimal digits for percentage values. Defaults to 2.</p> <code>2</code> Source code in <code>aimet_ml/utils/plots.py</code> <pre><code>def add_bar_label(bar_chart: plt.Axes, with_percent: bool = False, percent_digits: int = 2) -&gt; None:\n    \"\"\"\n    Add labels to a bar chart with optional percentage values.\n\n    Args:\n        bar_chart (plt.Axes): The bar chart object.\n        with_percent (bool, optional): Whether to include percentage values. Defaults to False.\n        percent_digits (int, optional): Number of decimal digits for percentage values. Defaults to 2.\n    \"\"\"\n    containers = bar_chart.containers[0]\n    labels = None\n    if with_percent:\n        datavalues = containers.datavalues\n        total = sum(datavalues)\n        labels = [f\"{v:,} ({v/total*100:.{percent_digits}f}%)\" for v in datavalues]\n    bar_chart.bar_label(containers, labels)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.plots.plt2arr","title":"<code>plt2arr(fig, draw=True)</code>","text":"<p>Convert a Matplotlib figure to a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The Matplotlib figure to be converted.</p> required <code>draw</code> <code>bool</code> <p>Whether to draw the figure. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The converted figure as a NumPy array.</p> Source code in <code>aimet_ml/utils/plots.py</code> <pre><code>def plt2arr(fig: Figure, draw: bool = True) -&gt; np.ndarray:\n    \"\"\"\n    Convert a Matplotlib figure to a NumPy array.\n\n    Args:\n        fig (Figure): The Matplotlib figure to be converted.\n        draw (bool, optional): Whether to draw the figure. Defaults to True.\n\n    Returns:\n        np.ndarray: The converted figure as a NumPy array.\n    \"\"\"\n    if draw:\n        fig.canvas.draw()\n    rgba_buf = fig.canvas.buffer_rgba()\n    (w, h) = fig.canvas.get_width_height()\n    rgba_arr = np.frombuffer(rgba_buf, dtype=np.uint8).reshape((h, w, 4))\n    return rgba_arr\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.plots.set_font","title":"<code>set_font(font_path)</code>","text":"<p>Set the font for Matplotlib using the provided font file.</p> <p>Parameters:</p> Name Type Description Default <code>font_path</code> <code>str</code> <p>Path to the font file.</p> required Source code in <code>aimet_ml/utils/plots.py</code> <pre><code>def set_font(font_path: str) -&gt; None:\n    \"\"\"\n    Set the font for Matplotlib using the provided font file.\n\n    Args:\n        font_path (str): Path to the font file.\n    \"\"\"\n    import matplotlib\n    import matplotlib.font_manager\n\n    font_prop = matplotlib.font_manager.FontProperties(fname=font_path)\n    matplotlib.font_manager.fontManager.addfont(font_path)\n    matplotlib.rc(\"font\", family=font_prop.get_name())\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.plots.set_thai_font","title":"<code>set_thai_font()</code>","text":"<p>Set the Thai font for Matplotlib using a predefined font path.</p> Source code in <code>aimet_ml/utils/plots.py</code> <pre><code>def set_thai_font() -&gt; None:\n    \"\"\"Set the Thai font for Matplotlib using a predefined font path.\"\"\"\n    font_path = str(PWD.parent / \"resources\" / \"fonts\" / \"thsarabunnew-webfont.ttf\")\n    set_font(font_path)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.wandb_utils","title":"<code>wandb_utils</code>","text":""},{"location":"api/utils/#aimet_ml.utils.wandb_utils.list_artifact_names","title":"<code>list_artifact_names(api, artifact_type, with_versions=True, with_aliases=True, per_page=100)</code>","text":"<p>List available artifact names for a specific artifact type.</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>Api</code> <p>The WandB API client.</p> required <code>artifact_type</code> <code>str</code> <p>The type of artifact for which names are listed.</p> required <code>with_versions</code> <code>bool</code> <p>Include version suffixes. Defaults to True.</p> <code>True</code> <code>with_aliases</code> <code>bool</code> <p>Include artifact aliases. Defaults to True.</p> <code>True</code> <code>per_page</code> <code>int</code> <p>Number of items to retrieve per page. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A sorted list of available artifact names with optional suffixes (versions or aliases).</p> Source code in <code>aimet_ml/utils/wandb_utils.py</code> <pre><code>def list_artifact_names(\n    api: wandb.Api, artifact_type: str, with_versions: bool = True, with_aliases: bool = True, per_page: int = 100\n) -&gt; list:\n    \"\"\"\n    List available artifact names for a specific artifact type.\n\n    Args:\n        api (wandb.Api): The WandB API client.\n        artifact_type (str): The type of artifact for which names are listed.\n        with_versions (bool, optional): Include version suffixes. Defaults to True.\n        with_aliases (bool, optional): Include artifact aliases. Defaults to True.\n        per_page (int, optional): Number of items to retrieve per page. Defaults to 100.\n\n    Returns:\n        list: A sorted list of available artifact names with optional suffixes (versions or aliases).\n    \"\"\"\n    available_artifact_names = set()\n\n    for collection in api.artifact_type(artifact_type).collections():\n        suffixes = set()\n\n        if with_aliases:\n            suffixes.update(collection.aliases)\n\n        if with_versions:\n            suffixes.update([v.version for v in collection.versions(per_page=per_page)])\n\n        if suffixes:\n            available_artifact_names.update([f\"{collection.name}:{suffix}\" for suffix in suffixes])\n        else:\n            available_artifact_names.update([collection.name])\n\n    return sorted(available_artifact_names)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.wandb_utils.load_artifact","title":"<code>load_artifact(api, artifact_type, artifact_name, artifact_alias, per_page=100)</code>","text":"<p>Load a WandB artifact by name and alias.</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>Api</code> <p>The WandB API client.</p> required <code>artifact_type</code> <code>str</code> <p>The type of artifact to load.</p> required <code>artifact_name</code> <code>str</code> <p>The base name of the artifact.</p> required <code>artifact_alias</code> <code>str</code> <p>The alias of the artifact.</p> required <code>per_page</code> <code>int</code> <p>Number of items to retrieve per page. Defaults to 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>Union[Artifact, None]</code> <p>wandb.Artifact: The loaded WandB artifact or None if it doesn't exist.</p> Source code in <code>aimet_ml/utils/wandb_utils.py</code> <pre><code>def load_artifact(\n    api: wandb.Api, artifact_type: str, artifact_name: str, artifact_alias: str, per_page: int = 100\n) -&gt; Union[wandb.Artifact, None]:\n    \"\"\"\n    Load a WandB artifact by name and alias.\n\n    Args:\n        api (wandb.Api): The WandB API client.\n        artifact_type (str): The type of artifact to load.\n        artifact_name (str): The base name of the artifact.\n        artifact_alias (str): The alias of the artifact.\n        per_page (int, optional): Number of items to retrieve per page. Defaults to 100.\n\n    Returns:\n        wandb.Artifact: The loaded WandB artifact or None if it doesn't exist.\n    \"\"\"\n    available_artifact_types = [t.name for t in api.artifact_types()]\n    if artifact_type not in available_artifact_types:\n        return None\n\n    available_artifact_names = list_artifact_names(api, artifact_type, per_page=per_page)\n\n    artifact_name_with_alias = f\"{artifact_name}:{artifact_alias}\"\n\n    if artifact_name_with_alias not in available_artifact_names:\n        return None\n\n    return wandb.use_artifact(artifact_name_with_alias)\n</code></pre>"},{"location":"api/utils/#aimet_ml.utils.wandb_utils.table_to_dataframe","title":"<code>table_to_dataframe(table)</code>","text":"<p>Convert a WandB table to a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The WandB table to be converted.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A Pandas DataFrame containing the data from the WandB table.</p> Source code in <code>aimet_ml/utils/wandb_utils.py</code> <pre><code>def table_to_dataframe(table: wandb.Table) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a WandB table to a Pandas DataFrame.\n\n    Args:\n        table (wandb.Table): The WandB table to be converted.\n\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the data from the WandB table.\n    \"\"\"\n    return pd.DataFrame(data=table.data, columns=table.columns)\n</code></pre>"}]}